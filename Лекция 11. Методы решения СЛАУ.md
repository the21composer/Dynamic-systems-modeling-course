# Методы решения систем линейных алгебраических уравнений

### Постановка задачи

Необходимо решить уравнение $A\textbf{x - b} = 0$, где

$A \in \mathbb{R}_{n\times n}$ - квадратная вещественная матрица

$\textrm{rank} (A)=n$ - матрица также невырожденная

##### Частные случаи

* Симметричная матрица $A^\text{T}=A$
* Положительно определенная матрица: $\mathbf{x^T}A\mathbf{x}>0, \forall \mathbf{x} \in \mathbb{R}^n, \mathbf{x} \neq 0$

### Задачи, в которых появляются СЛАУ

#### Моделирование жидкостей

<img src=".\sources\LETI6\cfd.png" alt="cfd" style="zoom:25%;" />

Это математическое гидродинамическое уравнение, которое должно выполняться в любой точке некоторого пространства, где течет некоторая идеальная математическая жидкость. Чтобы с такими уравнениями работать на компьютере, пространство разбивается:

<img src=".\sources\LETI6\cc-grid.png" alt="cc-grid" style="zoom: 50%;" />

В таком случае речь идет уже о выполнении не этих уравнений в каждой точке, а выполнение некоторой дискретизированной форме этого уравнения в каждой вершине этой сетки, или в каждом объеме.

В случае сетки, которая приведена на рисунке выше, неизвестными будут числовые характеристики жидкости в вершинах сетки (например, давление). Уравнений должно быть столько же, сколько неизвестных, и уравниваются другие числовые характеристики: потоки, проходящие между границами некоторых контрольных "подобъемов".

В итоге, получается система линейных уравнений, которую нужно решить на каждом шаге симуляции.

<img src=".\sources\LETI6\fvm-matrix.png" alt="fvm-matrix" style="zoom: 33%;" />

Матрица квадратная, причем $n$ равно количеству вершин в сетке, или количеству ячеек (иногда удвоенному или утроенному их количеству). Элементы матрицы численно характеризуют значения переменных в одних регионах сетки на поток жидкости в других регионах. Матрица получается разреженной, потому что давление жидкости в одном месте практически не влияет на поток жидкости в другом месте.

Разреженная матрица характеризуется большим количеством нулей, для них есть специализированные методы хранения и работы. Стоит отметить, что в этой задаче матрица не симметричная

#### Решение систем нелинейных уравнений

<img src=".\sources\LETI6\fx0.png" alt="fx0" style="zoom:25%;" />

Функцию можно разложить в ряд Тейлора в окрестности точки $\mathbf{x}$:

<img src=".\sources\LETI6\fx-taylor-2.png" alt="fx-taylor-2" style="zoom:25%;" />

Здесь $\nabla \mathbf{f(x)}$ - градиент функции:

<img src=".\sources\LETI6\gradf.png" alt="gradf" style="zoom:25%;" />

Поскольку функция $n$-значная и $\mathbf{x}$ - вектор длины $n$, то матрица получается квадратной. Эту матрицу еще называют *якобиан*.

##### Метод Ньютона для функции многих переменных

<img src=".\sources\LETI6\mv-newton-2.png" alt="mv-newton-2" style="zoom:25%;" />

Задача - получить $\mathbf{x}^{(n+1)}$ - приближение корня на следующем шаге. Полученное уравнение как раз представлено в формате: 

$A\textbf{x + b} = 0$

Несложно заметить, что если $n=1$, то уравнение приходит методу Ньютона функции от одной переменной:

<img src=".\sources\LETI6\newton-1d.png" alt="newton-1d" style="zoom:25%;" />

В случае, если якобиан у функции везде одинаковый (то есть функция является линейной), метод Ньютона для функции многих переменных может сойтись за один шаг.

#### Нелинейный метод наименьших квадратов

Рассмотрим пример нахождения корня нелинейной функции, который проистекает из необходимости найти минимум другой нелинейной функции. Допустим, мы строим модель какого-то процесса или явления, и необходимо подогнать параметры модели с тем, чтобы сократить по возможности невязки между экспериментальными данными (точки на графике) и модельными (кривая).

<img src=".\sources\LETI6\fit-sine.png" alt="fit-sine" style="zoom:50%;" />

Невязки - разности значений модели и наблюдений в точках, а минимизируется сумма квадратов этих разностей.

<img src=".\sources\LETI6\min-s.png" alt="min-s" style="zoom:25%;" />

Необходимо найти такой вектор параметров модели $\mathbf{x}$, при котором сумма квадратов невязок будет минимальна. Мы считаем, что этот минимум один. Если функция имеет минимум и она гладкая, то в этой точке ее градиент равен 0.

<img src=".\sources\LETI6\lsm1a.png" alt="lsm1a" style="zoom:25%;" />

Таким образом, получается система $n$ уравнений, которую необходимо решить. Для удобства, введем матрицу $A$:

<img src=".\sources\LETI6\lsm-2.png" alt="lsm-2" style="zoom:25%;" />

Тогда получаем матричную форму записи:

<img src=".\sources\LETI6\lsm-atr.png" alt="lsm-atr" style="zoom:25%;" />

Стоит заметить, что матрица не квадратная: как правило, число наблюдений больше, чем число параметров.

Чтобы применить метод Ньютона, надо посчитать градиент функции $f$:

<img src=".\sources\LETI6\lsm-dfi.png" alt="lsm-dfi" style="zoom:25%;" />

Здесь можно отбросить второй член суммы, и тогда аппроксимация якобиана:

<img src=".\sources\LETI6\lsm-ata.png" alt="lsm-ata" style="zoom:25%;" />

Остается записать уравнение метода Ньютона:

<img src=".\sources\LETI6\lsm-3.png" alt="lsm-3" style="zoom:25%;" />

### Классификация методов решения СЛАУ

* Прямые методы
  * LU-разложение (A не обязательно квадратная)
  * QR-разложение (A не обязательно квадратная)
  * LDL-разложение (A симметричная)
  * Разложение Холецкого (A симметричная, положительно определенная)
  * Мультифронтальное LU-разложение
  * Диагональные методы
    * Метод прогонки (A трехдиагональная)
    * SPIKE (A ленточная)
* Итеративные методы
  * Методы неподвижной точки
    * Метод Гаусса-Зейделя (основан на принципе сжимающего отображения)
    * Метод релаксации (обобщение метода Гаусса-Зейделя)
  * Методы, основанные на подпространствах Крылова
    * Метод сопряженных градиентов (A симметричная, положительно определенная)
    * Стабилизированный метод бисопряженных градиентов

*Прямые методы* отличаются тем, что выполняют фиксированное количество операций и выдают результат с максимальной доступной для данного метода точностью

*Итеративные методы* не имеют фиксированного количества времени выполнения, потому что оно зависит от количества итераций. С каждой итерацией значение $x$ все ближе и ближе к искомому $x^*$.

Итеративные методы зачастую более эффективны и часто применяются для решения систем с очень большим числом неизвестных (особенно если их миллион или больше)

##### Иллюстрация многообразия применяемых методов

Предположим, пользователь пишет в Mathlab: `x = A \ b`

Документация представляет собой разветвленную систему проверок, причем из двух деревьев.

<img src=".\sources\LETI6\matlab.png" alt="matlab" style="zoom:67%;" />          <img src=".\sources\LETI6\matlab2.png" alt="matlab2" style="zoom:67%;" />

Первое дерево работает в случае, если `x` - неразреженная матрица, а во втором случае - разреженная.

В каждом случае проверяется:

* Является ли матрица верхней треугольной / нижней треугольной
* Является ли диагональной / ленточной
* Является ли симметричной

И так далее...

И в зависимости от того, какая это матрица, выбирается тот или иной специализированный метод.

### LU-разложение

Метод работает в два этапа.

На первом этапе матрица $A$ раскладывается следующим образом:

<img src=".\sources\LETI6\lu.png" alt="lu" style="zoom:25%;" />

На втором этапе мы разбиваем задачу решения исходной системы на два последовательных этапа:

<img src=".\sources\LETI6\lux.png" alt="lux" style="zoom:25%;" />

Такое представление полезно тем, что системы по отдельности решаются проще.		

  * Решение первой системы

<img src=".\sources\LETI6\lyb.png" alt="lyb" style="zoom:25%;" />                                 <img src=".\sources\LETI6\sol-y.png" alt="sol-y" style="zoom:25%;" />

* Решение второй системы

<img src=".\sources\LETI6\uxy.png" alt="uxy" style="zoom:25%;" />                                <img src=".\sources\LETI6\x-sol.png" alt="x-sol" style="zoom:25%;" />

##### Как происходит само LU-разложение?

Алгоритм итеративный. Используем нотацию не совсем математическую: далее знак $\leftarrow$ означает присвоение переменной значения.

Начало:   <img src=".\sources\LETI6\liua.png" alt="liua" style="zoom:25%;" />

Шаг $i$: 

<img src=".\sources\LETI6\ujk.png" alt="ujk" style="zoom:25%;" />

<img src=".\sources\LETI6\lji.png" alt="lji" style="zoom:25%;" />

<img src=".\sources\LETI6\lu-step.png" alt="lu-step" style="zoom:25%;" />

На практике при реализации матрицы $L$ и $U$ можно сложить в одну матрицу (так как единицы на диагонали матрицы $L$ будут там всегда, их необязательно держать в памяти). Более того, матрицы можно хранить в том же месте, где хранилась матрица $A$.  

Схема с замещением, шаг $i$:

<img src=".\sources\LETI6\aji.png" alt="aji" style="zoom:25%;" />

Также в алгоритме содержится деление. Во-первых, знаменатели могут быть равны 0. Для гарантированной работы алгоритма матрица $A$ должна иметь невырожденные главные миноры.

### LU-разложение с перестановками (LUP)

Пусть есть некоторая перестановка $\sigma$:

<img src=".\sources\LETI6\sigma.png" alt="sigma" style="zoom:25%;" />

Тогда матрица этой перестановки $P_\sigma$:

<img src=".\sources\LETI6\psigma.png" alt="psigma" style="zoom:25%;" />

* $PA$ - (умножение слева) перестановка строк
* $AP$ - (умножение справа) перестановка столбцов 

LU-разложение с частичным выбором: $PA=LU$ (работает с любой невырожденной $A$)

<img src=".\sources\LETI6\ui.png" alt="ui" style="zoom:25%;" />

Если $u_{ii} = 0$, то можно переставить местами строки. В таком случае меняются столбцы в матрице $P$, чтобы инвариант сохранялся. Изначально матрица $P$ является единичной матрицей. Таким образом решается проблема деления на 0.

Итак, решим систему уравнений с разложением $PA=LU$. Все так же разбиваем систему на две:

<img src=".\sources\LETI6\pax.png" alt="pax" style="zoom:25%;" />

LU-разложение с полным выбором: $PAQ=LU$

* $P$ отвечает за перестановку строк
* $Q$ отвечает за перестановку столбцов

<img src=".\sources\LETI6\paqq.png" alt="paqq" style="zoom:25%;" />

Отметим, что матрица перестановки ортогональна, поэтому $QQ^T=I$

### Разложение Холецкого

Существует для симметричных положительно определенных матриц. Для таких матриц существует разложение вида

<img src=".\sources\LETI6\allt.png" alt="allt" style="zoom:25%;" />

Если такое разложение существует, то можно представить исходную систему в виде двух:

<img src=".\sources\LETI6\axb-chol.png" alt="axb-chol" style="zoom:25%;" />

Так как матрица $L$ - нижняя треугольная, то обе системы решаются последовательным исключением неизвестных.

Алгоритм разложения проще, чем алгоритмы $LU$ разложения, основан на рекурсивных отношениях, которые связывают $A$ и $L$:

<img src=".\sources\LETI6\chol.png" alt="chol" style="zoom:25%;" />

Так как матрица положительно определенная, то элементы под корнем не могут быть отрицательными.

Алгоритм быстрее, чем $LU$ (асимптотически за то же кубическое время, но число операций меньше), численно устойчив, перестановки не требуются.

В методе наименьших квадратов также используются симметричные положительно определенные матрицы, в связи с чем разложение Холецкого часто используется в задачах минимизации, оптимизации, подгонки параметров.

### Метод Гаусса-Зейделя

Если предыдущие методы были прямые, то метод Гаусса-Зейделя является итеративным.

Матрица $A$ выражается как сумма нижней треугольной и строго верхней треугольной:

<img src=".\sources\LETI6\alu.png" alt="alu" style="zoom:25%;" />

Система уравнений приводится к виду:

<img src=".\sources\LETI6\gz.png" alt="gz" style="zoom:25%;" />

Пусть есть начальное приближение $x^{(0)}$, тогда итеративная схема выглядит следующим образом:

<img src=".\sources\LETI6\gz-iter.png" alt="gz-iter" style="zoom:25%;" />

Последовательность обязательно сходится, когда отображение является сжимающим, то есть:

<img src=".\sources\LETI6\l-1u.png" alt="l-1u" style="zoom:25%;" />

Каждый шаг метода Гаусса-Зейделя делает количество операций, пропорциональное $n^2$. Таким образом, если шагов не очень много, метод будет работать эффективнее вышерассмотренных прямых методов.

### Метод сопряженных градиентов

Итеративный метод, условие - матрица $A$ симметричная положительно определенная.

В методе задача рассматривается не как задача уравнения, а как задача минимизации квадратичной формы:

<img src=".\sources\LETI6\fx-conj.png" alt="fx-conj" style="zoom:25%;" />

$f$ имеет единственный локальный минимум (при симм. полож. опред. $A$)

<img src=".\sources\LETI6\gradf-conj.png" alt="gradf-conj" style="zoom:25%;" />

##### Общая идея метода

Рассмотрим ход итерации в методе сопряженных градиентов как спуск некоторой "воронки" от точки $x_0$ к минимуму $x$.

<img src=".\sources\LETI6\conjgrad.png" alt="conjgrad" style="zoom: 20%;" />

Вычисляется градиент в $x_0$:

<img src=".\sources\LETI6\r0.png" alt="r0" style="zoom:25%;" />

Чтобы идти к минимуму функции нужно двигаться в противоположную от него сторону:

<img src=".\sources\LETI6\x1-cg.png" alt="x1-cg" style="zoom:25%;" />

Здесь $\alpha_0$ - это некоторый шаг - он определяет расстояние, которое нужно пройти в противоположную сторону от градиента. Метод должен двигаться в эту сторону до тех пор, пока функция не начнет возрастать. Выбор такого шага - решение минимизации функции относительно $\alpha_0$.

<img src=".\sources\LETI6\fx1.png" alt="fx1" style="zoom:25%;" />

<img src=".\sources\LETI6\df-dalpha0.png" alt="df-dalpha0" style="zoom:25%;" />

<img src=".\sources\LETI6\alpha0.png" alt="alpha0" style="zoom:25%;" />

Таким образом за один шаг можно существенно приблизиться к минимуму за счет оптимального выбора $\alpha_0$. Если такую же схему применять на дальнейших шагах, то получился бы метод градиентного спуска (розовый на графике), но метод сопряженных градиентов (бирюзовый на графике) работает иначе и работает гораздо быстрее.

##### Шаги метода

На первом шаге задается вектор $\mathbf{p}^{(0)}$:

<img src=".\sources\LETI6\p0r0.png" alt="p0r0" style="zoom:25%;" />

На каждом шаге вычисляется невязка:

<img src=".\sources\LETI6\ri-krylov.png" alt="ri-krylov" style="zoom:25%;" />

Направление движения определяется вектором $\mathbf{p^{(i)}}$, который может существенно отличаться от обратного направления градиенту в текущей точке:

<img src=".\sources\LETI6\xip1-alphai.png" alt="xip1-alphai" style="zoom:25%;" />

Меняется и вычисление $\alpha_i$:

<img src=".\sources\LETI6\alpha-i.png" alt="alpha-i" style="zoom:25%;" />

Как вычисляется направление спуска? Для этого применяются подпространства Крылова - это линейная оболочка всех невязок с 0 шага до $i$-го:

<img src=".\sources\LETI6\ki.png" alt="ki" style="zoom:25%;" />

Очередной $x^{(i+1)}$ ищется по всему подпространству, то есть:

<img src=".\sources\LETI6\xip1-ki.png" alt="xip1-ki" style="zoom:25%;" />

Для базиса подпространства будут использоваться $\mathbf{p}^{(i)}$. При этом необходимо задать условия:

<img src=".\sources\LETI6\pip1.png" alt="pip1" style="zoom:25%;" />

Итоговая формула для $\mathbf{p}^{(i+1)}$:
$$
\mathbf{p}^{(i+1)} = \mathbf{r}^{(i+1)} + \dfrac{\mathbf{r}^{(i+1)^T} \mathbf{r}^{(i+1)}}{\mathbf{r}^{(i)^T} \mathbf{r}^{(i)}}\mathbf{p}^{(i)}
$$
Подпространство Крылова ограничено размером вектора $\mathbf{x}$, то есть алгоритм не будет делать больше чем $n$ шагов, а как правило, делает намного меньше. В системе, где $n=1000000$, метод может сойтись за, например, 10 или 20 шагов. Это делает его очень применимым в реальных задачах с большими матрицами. 

В алгоритме всего два деления на шаг, что делает метод удобным для реализации работы с разряженными матрицами.

### Другие методы

* QR-разложение
* SVD-разложение
* LDL-разложение - родственно разложению Холецкого, предназначено для симметричных матриц (нет условия положительной определенности)
* Блочные алгоритмы - работают с матрицами как с наборами квадратных блоков, что актуально для параллельных вычислений
* Алгоритмы для разреженных матриц
* Мультифронтальные LU (MUMPS, UMFPack) - хорошо работает для разреженных матриц, в отличие от обычного LU
* BiCG, BiCGSTAB - модификации метода сопряженных градиентов, которые работают с любыми невырожденными матрицами 
* Предобуславливатели

