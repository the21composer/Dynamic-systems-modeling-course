# Методы Адамса

Методы Адамса являются, в отличие от одношаговых методов Рунге-Кутты, многошаговыми.

### Задача (многошаговый метод общего вида)

Пусть в некоторый момент $t$ известно состояние системы $\mathbf{x}(t)$. Требуется найти состояние системы на следующем шаге, то есть $\mathbf{x}(t+h)$. При этом, в многошаговых методах также известны состояния системы на предыдущих $k - 1$ шагах.
$$
\left.\begin{array}{ll}
\mathbf{x}(t), &\mathbf{f}(\mathbf{x}(t))\\
\mathbf{x}(t-h), &\mathbf{f}(\mathbf{x}(t-h))\\
\mathbf{x}(t-2h), &\mathbf{f}(\mathbf{x}(t-2h))\\
\ldots\\
\mathbf{x}(t-(k-1)h), &\mathbf{f}(\mathbf{x}(t-(k-1)h))\\
\end{array}\right\} \xrightarrow{\mathbf{f}} \mathbf{x}(t+h)
$$
Здесь $\dot{\mathbf{x}} = \textbf{f}(\mathbf{x})$.

Введем упрощающую нотацию. Пусть $x_n$ — состояние системы на текущем шаге, $\textbf{f}_n$ — вектор правой части для этого состояния. Получаем запись:
$$
\left.\begin{array}{ll}
\mathbf{x}_n, &\mathbf{f}_n\\
\mathbf{x}_{n-1}, &\mathbf{f}_{n-1}\\
\mathbf{x}_{n-2}, &\mathbf{f}_{n-2}\\
\ldots\\
\mathbf{x}_{n-k+1}, &\mathbf{f}_{n-k+1}\\
\end{array}\right\} \xrightarrow{\mathbf{f}} \mathbf{x}_{n+1}
$$
Так же как и в методах Рунге-Кутты, функция $\textbf{f}$ может вызываться неограниченное число раз.

### Задача (метод Адамса)

Ранее был рассмотрен общий вид многошаговых методов. В методе Адамса же используются предыдущие значения $\mathbf{x}_{n-1}, \ldots, \mathbf{x}_{n-k+1}$. Записать это можно так:
$$
\left.\begin{array}{ll}
\mathbf{x}_n, &\mathbf{f}_n\\
 &\mathbf{f}_{n-1}\\
 &\mathbf{f}_{n-2}\\
& \ldots\\
\phantom{\mathbf{x}_{n-k+1},} &\mathbf{f}_{n-k+1}\\
\end{array}\right\} \xrightarrow{\mathbf{f}} \mathbf{x}_{n+1}
$$
В основе методов Адамса лежит экстраполяция (или интерполяция) функции $\textbf{f}$, то есть правой части.

Допустим, есть истинная функция $\textbf{f}$, проходящая через точки $(t_{n-k+1}, \textbf{f}_{n-k+1}), ..., (t_n, \textbf{f}_{n})$. Функцию можно аппроксимировать полиномом, который также проходит через эти точки, ожидая, что на интервале $t_n, t_{n+1}$ поведение полинома не будет сильно отличаться от поведения функции. 

> Значение следующего $\mathbf{x}_{n+1}$ получается путем интегрирования функции $\textbf{f}$ на всем промежутке от $t_n, t_{n+1}$

##### Экстраполяция $\textbf{f}$ на основе $k$ предыдущих значений

Обратные разделенные разности:
$$
\begin{array}{lcl}
[\mathbf{f}_n] &=& \mathbf{f}_n \\
{} [\mathbf{f}_n\ \ldots\ \mathbf{f}_{n-i}] &=& \frac{[\mathbf{f}_n\ \ldots\ \mathbf{f}_{n-i+1}] - [\mathbf{f}_{n-1}\ \ldots\ \mathbf{f}_{n-i}]}{t_n - t_{n-i}}\\
\end{array}
$$
Так, обратная разделенная разность 0-го порядка в точке $\textbf{f}_n$ равна значению $\textbf{f}_n$, а разности высших порядков вычисляются рекурсивно.

С помощью обратных разделенных разностей можно записать интерполяционный многочлен в форме Ньютона:
$$
\begin{array}{ll}
\mathbf{f}(t) \approx & [\mathbf{f}_n]\ + \\
& [\mathbf{f}_n\ \mathbf{f}_{n-1}] (t-t_n)\ + \\
& [\mathbf{f}_n\ \mathbf{f}_{n-1}\ \mathbf{f}_{n-2}] (t-t_n)(t-t_{n-1})\ + \\
& \ldots \\
& [\mathbf{f}_n\ \ldots \mathbf{f}_{n-k+1}] (t-t_n)\ldots(t-t_{n-k+2})\\
\end{array}
$$
Равенство нестрогое, потому что многочлен является аппроксимацией истинной функции $\textbf{f}(t)$.

Введем также нотацию обратных конечных разностей:
$$
\begin{array}{lcl}
\nabla^0 \mathbf{f}_n &=& \mathbf{f}_n \\
{} \nabla^i \mathbf{f}_n &=& \nabla^{i-1} \mathbf{f}_n - \nabla^{i-1} \mathbf{f}_{n-1}\\
\end{array}
$$
Как видно, нотация похожа на обратные разделенные разности, так же рекурсивно задается. На самом деле, обратную конечную разность можно вывести из обратной разделенной и наоборот:

$t_m - t_{m-i} = ih \Rightarrow [\textbf{f}_n \ ... \ \textbf{f}_{n-i}] = \dfrac{\Delta^i \textbf{f}}{i!h^i}$

Перепишем интерполяционный полином с использованием обратных конечных разностей. Дополнительно введем переменную $s=\dfrac{t-t_n}{h}$
$$
\def\arraystretch{1.3}
\begin{array}{ll}
\mathbf{f}(t) \approx & \nabla^0 \mathbf{f}_n\cdot 1\ + \\
& \frac{\nabla^1 \mathbf{f}_n}{h}\cdot h s\ + \\
& \frac{\nabla^2 \mathbf{f}_n}{2h^2}\cdot h^2 s(s+1)\ + \\
& \ldots \\
& \frac{\nabla^{k-1}\mathbf{f}_n }{(k-1)!h^{k-1}}\cdot h^{k-1} s\ldots(s+k-2)\\
\end{array}
$$

##### Интеграл правой части

Прежде чем взять интеграл, перепишем функцию в виде суммы:
$$
\mathbf{f}(t) \approx  \sum\limits_{i=0}^{k-1}\frac{1}{i!} \nabla^i \mathbf{f}_n\cdot 1\cdot s(s+1)\ldots(s+i-1)
$$
Проинтегрируем функцию от $t_n$ до $t_{n+1}$:
$$
\mathbf{x}_{n+1} = \mathbf{x}_n + \int\limits_{t_n}^{t_{n+1}} \mathbf f(\mathbf{x}(t)) \mathrm{d}t
$$

$$
\begin{array}{l}
\approx \mathbf{x}_n + h\int\limits_{0}^{h}  \sum\limits_{i=0}^{k-1}\frac{1}{i!} \nabla^i \mathbf{f}_n\cdot 1\cdot s(s+1)\ldots(s+i-1) \mathrm{d}s\\
= \mathbf{x}_n + h\sum\limits_{i=0}^{k-1}\gamma_i \nabla^i \mathbf{f}_n\\
\end{array}
$$

Здесь вводятся коэффициенты $\gamma_i$:
$$
\gamma_i = \frac{1}{i!}\int_{0}^{1} 1\cdot s(s+1)\ldots(s+i-1) \mathrm{d}s
$$

>  Подчеркнем, что при $i=0$ под интегралом только $1$. Соответственно, при $i=1$ под интегралом $s$ и так далее.

Чтобы вывести эти коэффициенты (которые для каждого $i$ являются константой), введем понятие *биномиального коэффициента (обобщенного)*:
$$
\frac{(-s)(-s-1)(-s-2)\ldots(-s-i+1)}{i!} \equiv \left(\begin{array}{c}-s \\ i\end{array}\right)
$$
Тогда коэффициенты $\gamma_i$ можно записать следующим образом:
$$
\gamma_i = \frac{1}{i!}\int_{0}^{1} 1\cdot s(s+1)\ldots(s+i-1) \mathrm{d}s
 = (-1)^i \int_{0}^{1} \left(\begin{array}{c}-s \\ i\end{array}\right) \mathrm{d}s
$$
$(-1)^i$ появилась потому, что каждая скобка была умножена на $-1$ для соответствия биномиальному коэффициенту.

Чтобы посчитать $\gamma_i$, обратимся к методу производящей функции.

Рассмотрим такую функцию $G(u)$:
$$
G(u) \equiv \sum_{i=0}^\infty \gamma_i u^i = \int_{0}^{1} \sum_{i=0}^\infty 1^{-s-i} \left(\begin{array}{c}-s \\ i\end{array}\right) (-u)^i \mathrm{d}s
$$
Здесь сумма под интегралом – бином Ньютона, обобщенный на случай нецелой степени, поэтому
$$
=\int_0^1 (1-u)^{-s}\mathrm{d}s = -\frac{u}{(1-u)\ln(1-u)}
$$
Перепишем равенство в другом виде:
$$
-\frac{\ln(1-u)}{u}G(u)  = \frac{1}{1-u}
$$
Все сомножители равенства имеют выражение в рядах, и выражение принимает вид:
$$
\left(1+\frac{1}{2}u + \frac{1}{3}u^2 + \ldots\right)\left(\gamma_0 + \gamma_1 u + \gamma_2 u^2+\ldots\right)= \left(1+u+u^2+u^3+\ldots\right)
$$
Отсюда можно найти $\gamma_0, \gamma_1, ...$
$$
\def\arraystretch{1.2}
\begin{array}{lcl}
\gamma_0 &=& 1\\
\frac{1}{2}\gamma_0 + \gamma_1 &=& 1\\
\frac{1}{3}\gamma_0 + \frac{1}{2}\gamma_1 + \gamma_2 &=& 1\\
\ldots\\
\frac{1}{i+1}\gamma_0 + \ldots + \frac{1}{2}\gamma_{i-1} + \gamma_i &=& 1
\end{array}
$$
Решение этой системы уравнений можно представить в виде таблицы, которая как правило заложена в реализации методов:
$$
\def\arraystretch{1.2}
\begin{array}{|cccccc}\hline
\gamma^*_0 & \gamma^*_1 & \gamma^*_2 & \gamma^*_3 & \gamma^*_4 & \ldots\\ \hline
1 & \frac{1}{2} & \frac{5}{12} & \frac{3}{8} & \frac{251}{720} & \ldots \\ \hline
\end{array}
$$

##### Метод Адамса-Башфорта: итоговые формулы

Общая формула метода:
$$
\mathbf{x}_{n+1} \approx \mathbf{x}_n + h\sum\limits_{i=0}^{k-1}\gamma_i \nabla^i \mathbf{f}_n\\
$$
Формулы для $\textbf{x}_{n+1}$ при разном значении $k$ (метод в каждом случае называется $k$-шаговым):

$k=1: \mathbf{x}_{n+1} \approx \mathbf{x} + h \mathbf{f}_n$

$k=2: \mathbf{x}_{n+1} \approx \mathbf{x}_n + h \left( \dfrac{3}{2} \mathbf{f}_n - \dfrac{1}{2} \mathbf{f}_{n-1} \right)$

$k=3: \mathbf{x}_{n+1} \approx \mathbf{x}_n + h \left( \dfrac{23}{12} \mathbf{f}_n - \dfrac{4}{3} \mathbf{f}_{n-1} + \dfrac{5}{12} \mathbf{f}_{n-2} \right)$

$k=4: \mathbf{x}_{n+1} \approx \mathbf{x}_n + h \left( \dfrac{55}{24} \mathbf{f}_n - \dfrac{59}{24} \mathbf{f}_{n-1} + \dfrac{37}{24} \mathbf{f}_{n-2} - \dfrac{3}{8} \mathbf{f}_{n-3} \right)$

Стоит отметить, что методы Адамса неспособны работать без использования другого одношагового метода для «разгона». Разгон необходим потому, что метод Адамса подразумевает наличие $k-1$ предыдущих уже сделанных шагов. Соответственно, на первом шаге метода делается $k-1$ шагов одношаговым методом.

Методы Адамса-Башфорта основаны на экстраполяции функции $\mathbf{f}$ на один шаг дальше некоторого промежутка, где есть сохраненные ранее значения. Но экстраполяция работает хуже, чем интерполяция за пределами интервала, в котором имеются известные значения.

 ### Неявный метод Адамса: интерполяция функции правой части

Неявные методы отличаются тем, что используют в определении полинома, который интерполирует функцию $\mathbf{f}$, точку $t_{n+1}$.

Интерполяционный полином записывается таким образом и имеет $k+1$ слагаемых (запись с помощью обратных разностей):
$$
\def\arraystretch{1.3}
\begin{array}{ll}
\mathbf{f}(t) \approx & \nabla^0 \mathbf{f}_{n+1}\cdot 1\ + \\
& \frac{\nabla^1 \mathbf{f}_{n+1}}{1}\cdot (s-1)\ + \\
& \frac{\nabla^2 \mathbf{f}_{n+1}}{2}\cdot (s-1)s\ + \\
& \frac{\nabla^3 \mathbf{f}_{n+1}}{6}\cdot (s-1)s(s+1)\ + \\
& \ldots \\
& \frac{\nabla^{k}\mathbf{f}_{n+1} }{k!}\cdot (s-1)\ldots(s+k-2)\\
\end{array}
$$

#### Метод Адамса-Мултона: вывод коэффициентов

$$
\mathbf{x}_{n+1} = \mathbf{x}_n + \int\limits_{t_n}^{t_{n+1}} \mathbf f(\mathbf{x}(t)) \mathrm{d}t \approx \mathbf{x}_n + h\sum\limits_{i=0}^{k}\gamma^*_i \nabla^i \mathbf{f}_{n+1}\\
$$

Заметим, что сумма уже от $0$ до $k$ включительно, в отличие от явного метода. Также вводятся коэффициенты $\gamma_i^*$, которые определены следующим образом:
$$
\gamma^*_i = \frac{1}{i!}\int_{0}^{1} (s-1)s(s+1)\ldots(s+i-2) \mathrm{d}s
 = (-1)^i \int_{0}^{1} \left(\begin{array}{c}-s+1 \\ i\end{array}\right) \mathrm{d}s
$$
Теперь попробуем посчитать разность $\gamma_i - \gamma_{i-1}$ (без звездочек!)
$$
\gamma_i - \gamma_{i-1} = (-1)^i\int_0^1 \left(\begin{array}{c}-s\\ i\end{array}\right) +  \left(\begin{array}{c}-s\\ i - 1\end{array}\right)\mathrm{d}s(-1)^i = (-1)^i\int_0^1 \left(\begin{array}{c}-s+1\\ i\end{array}\right)ds = \gamma_i^*,\quad i>0
$$
Итак, коэффициенты $\gamma_i^*$ можно вывести из $\gamma_i$ по формуле выше, а значит можно аналогично составить таблицу:
$$
\def\arraystretch{1.2}
\begin{array}{|cccccc}\hline
\gamma^*_0 & \gamma^*_1 & \gamma^*_2 & \gamma^*_3 & \gamma^*_4 & \ldots\\ \hline
1 & -\frac{1}{2} & -\frac{1}{12} & -\frac{1}{24} & -\frac{19}{720} & \ldots \\ \hline
\end{array}
$$

##### Предиктор и корректор

Один из методов решения неявных уравнений вида, который задан формулой $\textbf{x}_{n+1}$ — это метод неподвижной точки, которому однако необходимо начальное приближение точки $\textbf{x}_{n+1}$, для чего можно использовать метод Адамса-Башфорта.

Используем нотацию не совсем математическую: далее знак $\leftarrow$ означает присваивание переменной значения.

$\mathbf{x}_{n+1} \leftarrow \mathbf{x}_n + h\sum\limits_{i=0}^{k-1}\gamma_i \nabla^i \mathbf{f}_n$ :   Predictor (P)

$\mathbf{f}_{n+1} \leftarrow \mathbf{f}(\mathbf{x}_{n+1})$ :   Evaluator (PE)

Итак, теперь можно использовать формулу метода Адамса-Мултона:

$\mathbf{x}_{n+1} \leftarrow \mathbf{x}_n + h\sum\limits_{i=0}^{k}\gamma^*_i \nabla^i \mathbf{f}_{n+1}$ :   Corrector (PEC)

Можно предпринимать и дальнейшие шаги:

$\mathbf{f}_{n+1} \leftarrow \mathbf{f}(\mathbf{x}_{n+1})$ (PECE)

$\mathbf{x}_{n+1} \leftarrow \mathbf{x}_n + h\sum\limits_{i=0}^{k}\gamma^*_i \nabla^i \mathbf{f}_{n+1}$ (PECEC)

И так далее. Как правило, таких шагов не требуется слишком много, итерации сходятся за 3-4 шага.

Изложенная схема является самодостаточной реализацией неявного метода Адамса, не считая того, что первые $k-1$ значений $\mathbf{f}_i$ должны быть, как и в явном методе Адамса, предварительно рассчитаны одношаговым методом.

##### Оптимизация вычислений

Для оптимизации вычислений можно упросить вычисления корректора. Повторим сначала всю схему предиктора и корректора:

$\mathbf{x}_{n+1} \leftarrow \mathbf{x}_n + h\sum\limits_{i=0}^{k-1}\gamma_i \nabla^i \mathbf{f}_n$ :  Predictor (P)

$\mathbf{f}_{n+1} \leftarrow \mathbf{f}(\mathbf{x}_{n+1})$ :  Evaluator (PE)

 $\mathbf{x}_{n+1} \leftarrow \mathbf{x}_n + h\sum\limits_{i=0}^{k}\gamma^*_i \nabla^i \mathbf{f}_{n+1}$ :  Corrector (PEC)

На шаге корректора можно ввести поправку, вычислив разность между суммами в корректоре и предикторе:
$$
\begin{array}{l}
\sum\limits_{i=0}^{k}(\gamma_i-\gamma_{i-1}) \nabla^i \mathbf{f}_{n+1} - \sum\limits_{i=0}^{k-1}\gamma_i \nabla^i \mathbf{f}_n = \ldots\\
\ldots = \gamma_k \nabla^k \mathbf{f}_{n+1}
\end{array}
$$
Таким образом, корректор может работать намного проще:
$$
\mathbf{x}_{n+1} \leftarrow \mathbf{x}_{n+1} + h\gamma_k \nabla^k \mathbf{f}_{n+1}
$$
Используя такой корректор, можно значительно поднять производительность реализации метода.

#### Оценка ошибки методов Адамса

Оценка ошибки выводится из оценки ошибки интерполяции или экстраполяции функции $\mathbf{f}$. 

Рассмотрим на примере метода Адамса-Башфорта, то есть на примере экстраполяции (для интерполяции схема такая же).

Посчитаем разность между функцией и ее аппроксимацией:
$$
\mathbf{f}(t) - \sum\limits_{i=0}^{k-1}\frac{\nabla^i \mathbf{f}_n}{i!h^i} (t-t_n)\ldots(t-t_{n-i+1}) = \frac{\mathbf{f}^{(k)}(\xi)}{k!}\prod_{i=0}^{k-1}(t-t_{n-i}) \\
\begin{array}{l}
\xi \geqslant \min(t_{n-k+1},\ldots,t_{n},t)\\
\xi \leqslant \max(t_{n-k+1},\ldots,t_{n},t)
\end{array}
$$
Если $\textbf{f}^{(k)}$ ограничена, то ошибка также ограничена:
$$
\mathbf{f}(t) - \sum\limits_{i=0}^{k-1}\frac{\nabla^i \mathbf{f}_n}{i!h^i} (t-t_n)\ldots(t-t_{n-i+1}) = \frac{\mathbf{f}^{(k)}(\xi)}{k!}\prod_{i=0}^{k-1}(t-t_{n-i})

\stackrel{\scriptstyle |\mathbf{f}^{(k)}| \leqslant M}{<} h^k \frac{M}{k!}\prod_{i=0}^{k-1}(s + i) = O(h^k)
$$

##### Локальная оценка ошибки явного метода

$$
\mathbf{x}_{n+1} - (\mathbf{x}_n + h\sum\nolimits_{i=0}^{k-1}\gamma_i \nabla^i \mathbf{f}_n) = \int_{t_n}^{t_{n+1}} O(h^k) \mathrm{d}t = o(h^{k})
$$

##### Локальная оценка ошибки неявного метода

$$
\mathbf{x}_{n+1} - (\mathbf{x}_n + h\sum\nolimits_{i=0}^{k}\gamma^*_i \nabla^i \mathbf{f}_n) = \int_{t_n}^{t_{n+1}} O(h^{k+1}) \mathrm{d}t = o(h^{k+1})
$$

Порядок интерполяции на 1 больше, так как она делается по $k+1$ точке.

Итак,

* $k$-шаговый метод Адамса-Башфорта имеет порядок $k$
* $k$-шаговый метод Адамса-Мултона имеет порядок $k+1$

### Заключение

Стоит сказать, что методы Адамса-Башфорта и Адамса-Мултона придумал именно Адамс, единолично. Они так названы для различимости.

Неявные методы Адамса-Мултона могут накапливать существенно меньшую ошибку, так как используют полиномиальную интерполяцию, а не экстраполяцию.

Общеизвестные источники приводят не самую эффективную реализацию. Для ускорения нужно:

* Применять эффективный метод пересчета конечных разностей
* Применять эффективный алгоритм корректора

Все методы Адамса не могут работать без одношагового разгона, для него можно использовать методы Рунге-Кутты, например, 4-го порядка или выше.

Выше рассматривалась интерполяция и экстраполяция на равномерной сетке, что может приводить к большим ошибкам. Известна функция Рунге, которая интерполируется тем хуже, чем меньше шаг. Для борьбы с этим можно применять неравномерные сетки, которые затруднительно применять в методах Адамса.

Существуют и другие многошаговые методы, например, метод BDF.

