# Численные методы решения нелинейных уравнений

В лекции будут рассматриваться уравнения вида $f(x^*) = 0$, где $x, f \in \mathbb{R}$, а также $x^* = \varphi(x^*)$. Подразумевается, что корень у функции один и существует. Кроме того подразумевается, что функция $f$ нелинейная.

Представление корня будет искаться итеративно. Так, будет некоторое начальное приближение $x_0$, а методы будут вычислять последовательность, которая сходится к $x^*$:

$\lim_{n\rightarrow\infty} x_n = x^*$ 

Найти точное значение корня в общем случае не представляется возможным, так как

* Точный корень может не иметь арифметического выражения
* В машинной арифметике присутствуют погрешности

Введем понятия прямой и обратной ошибки.

Прямая ошибка: $\Delta f = |f(x_n)|$ 

Обратная ошибка: $\Delta x = |x_n - x^*|, \epsilon_x = \Delta x / x$ (относительная обратная ошибка)

### Метод половинного деления

Метод деления пополам позволяет исключать в точности половину интервала на каждой итерации. 

При использовании метода считается, что функция непрерывна и имеет на концах интервала разный знак, что гарантирует наличие хотя бы одного корня на интервале.

> Метод каждый раз делит отрезок на два равных (добавляется новая серединная точка) и для следующей итерации выбирает тот, в котором значения функции на концах отличаются по знаку. Так продолжается до тех пор, пока в серединной точке значение функции не станет по модулю меньше, чем установленная погрешность.

Рассмотрим процесс построения последовательности. За начальную точку берется середина некоторого отрезка $[L_0, R_0]$, который удовлетворяет условию: $sign(f(L_0)) \neq sign(f(R_0))$.

<img src=".\sources\LETI5\x0l0r0.png" alt="x0l0r0" style="zoom:25%;" />

Далее возможны три случая на каждой итерации:

* Если $sign(f(L_n)) \neq sign(f(x_n))$

<img src=".\sources\LETI5\bisect1.png" alt="bisect1" style="zoom:25%;" />

* Если $sign(f(x_n)) \neq sign(f(R_n))$

  <img src=".\sources\LETI5\bisect2.png" alt="bisect2" style="zoom:25%;" />

* $x^*=x_n$

Метод половинного деления может работать достаточно долго, так как на каждом шаге отрезок, на котором может быть корень, уменьшается всего в два раза. Иначе говоря, обратная ошибка на шаге $n$:

<img src=".\sources\LETI5\bisect-err.png" alt="bisect-err" style="zoom:25%;" />

Такая сходимость является *линейной*, то есть *сходимостью порядка 1*.

При использовании мантиссы преставления корня размером 52 бит, в худшем случае потребуется 52 итерации, чтобы получить корень. Это медленно, поэтому метод применяется крайне редко.

### Метод простой итерации

Вспомним представление нелинейного уравнения как $x^*=\varphi(x^*)$. Построение последовательности в методе простой итерации эту формулу напоминает:

$x_{n+1} = \varphi(x_n)$

> Берется некоторое начальное приближение. На каждом следующем шаге применяем функцию $\varphi$ к предыдущей аппроксимации. Визуально это выглядит как "ступеньки" между функцией $y=\varphi(x)$ и $y=x$. Не всегда сходится.

<img src="D:\Github\Dynamic-systems-modeling-course\sources\LETI5\iter4types.png" alt="iter4types" style="zoom:50%;" />

Если функция $\varphi(x)$ - сжимающее отображение, то есть $\varphi'(x) \le k < 1$, то $\lim_{n \rightarrow \infty} x_n = x^*$.

##### Пример: уравнение Кеплера для эллипса

$E = M + e \sin E \ [= \varphi(E)], e<1$

$\varphi' = e \cos E \le e < 1$

$E$ - эксцентрическая аномалия

$M$ - средняя аномалия

Тогда метод будет рассчитывать последовательность:

<img src=".\sources\LETI5\kerpler-iter.png" alt="kerpler-iter" style="zoom:25%;" />

Иллюстрация "а" на рисунке 1.11 выше иллюстрирует работу метода на подобном уравнении.

Поговорим о сходимости метода.

 Учитывая, что $\varphi'(x) \le k < 1$, получаем, что

<img src=".\sources\LETI5\xnp1phixn-fix.png" alt="xnp1phixn-fix" style="zoom:25%;" />

Таким образом, последовательность убывает, то есть действительно  $\lim_{n \rightarrow \infty} x_n = x^*$.

*Порядок сходимости $q$:*

<img src=".\sources\LETI5\order-of-converg.png" alt="order-of-converg" style="zoom:25%;" />

* Если $\varphi'$ определена и непрерывна:

<img src=".\sources\LETI5\fp-lim1.png" alt="fp-lim1" style="zoom:25%;" />

Таким образом, порядок сходимости - 1.

* Если $\varphi'(x^*) = 0$, $\varphi''$ определена и непрерывна:

Используем разложение в ряд Тейлора:

<img src=".\sources\LETI5\phixn-exp.png" alt="phixn-exp" style="zoom:25%;" />

Тогда, полагая $q = 2$:

<img src=".\sources\LETI5\fp-lim2.png" alt="fp-lim2" style="zoom:25%;" />

Предел существует, значит при соблюдении вышеупомянутых условий порядок сходимости метода - 2.

### Метод Ньютона

Один из наиболее часто используемых методов, обладающий порядком сходимости 2.

Определим функцию $\varphi(x)$:

<img src=".\sources\LETI5\phi_newton.png" alt="phi_newton" style="zoom:25%;" />

Несложно заметить, что ее производная:

<img src=".\sources\LETI5\varphidot.png" alt="varphidot" style="zoom:25%;" />

Тогда, если $f'(x) \neq 0$: $\varphi(x^*) = x^*, \varphi(x^*) = 0$

##### Условия квадратичной сходимости $q = 2$

* $f'(x) \neq 0$, при нарушении метод не сможет продолжить работу
* $f''(x)$ непрерывна
* $f'''(x)$ ограничена
* Начальное приближение достаточно близко к $x^*$

Эти ограничения, однако, достаточно серьезные, но квадратичная скорость сходимости оправдывает применение метода.

При нарушении последний трех нарушений метод может либо не сойтись вообще, либо сойтись с меньшим порядком сходимости.

Графически один шаг метода выглядит так:

<img src=".\sources\LETI5\Newton_iteration.png" alt="Newton_iteration" style="zoom:33%;" />

> Метод проводит касательную к функции в точке $x_n$, а за $x_{n+1}$ берется точка, в которой эта касательная пересекается с 0

##### Примеры нарушений условий

1. $f(x) = \sqrt{x}: x_{n+1} = x_n - \dfrac{\sqrt{x_n}}{-1/(2\sqrt{x_n})} = -x_n$

Прежде всего, нарушается условие $f'(x) \neq 0$, так как $f'(x^*)=0, x^*=0$.

Если начинать итерации с 0, то продолжить их мы не сможем. С другой стороны, это и не потребуется, так как 0 есть корень.

Если начать итерации с положительного числа, метод на первом же шаге попадет в отрицательную область (где, в общем-то, значения функции уже не определены). Но метод этого "не заметит" и продолжит вычислять последовательность, которая будет состоять из двух чисел, одинаковых по модулю и разных по знаку. Метод расходится.

2. $f(x)=x^2: x_{n+1} = x_n - \dfrac{x_n^2}{2x_n} = \dfrac{x_n}{2}$

Если начинать итерации с 0, то продолжить их мы не сможем. С другой стороны, это и не потребуется, так как 0 есть корень.

Если начинать с положительного числа, то, как видно из выражения выше, на каждом шаге метод будет вычислять значение, вдвое меньшее значению на предыдущем шаге. Таким образом корень будет достигнут, но порядок сходимости - 1.

### Машинные вычисления

Машинные вычисления отличаются от вычислений на бумаге тем, что числа представляются в виде дробей со знаменателем, равным степени двойки. Таким образом, представление чисел уже несет в себе некоторую погрешность. Также, количество операций процессором ограничено.

Примитивы:

* fadd, fsub, fmul, fdiv (+, -, *, /)
* fsqrt ($\sqrt{x}$)
* fsin, fcos
* fyl2x, fy2lxp1 ($y \cdot \log_2(x), y \cdot \log_2(x+1)$)
* rsqrtss (SSE) ($1/ \sqrt{x}$)

На самом деле, в некоторых процессорах нет даже всех этих примитивов. В этом случае, например, $sin(x)$ можно посчитать через ряд Тейлора:

<img src=".\sources\LETI5\sin-exp.png" alt="sin-exp" style="zoom:25%;" />

При больших значениях $x$ требуется вычислить достаточно большой ряд, что не оптимально, поэтому $x$ переносится в окрестность 0, учитывая периодичность функции $\sin(x)$. 

В процессорах Intel была реализована такая операция уменьшения аргумента.

$\sin(\pi+\varepsilon) = - sin(\varepsilon)$

Но это было сделано без учета достаточного количества значащих цифр в числе $\pi$, и в 2014 году было обнаружено, что все реализации $\sin$ в процессорах Intel имеют ошибку, которая проявляется при аргументах, близких к $\pi$.

<img src=".\sources\LETI5\intel-fuckup.PNG" alt="intel-fuckup" style="zoom:66%;" />

В связи с этим, синус теперь всегда считается через ряд Тейлора.

##### Вычисление квадратного корня

Забудем, что такой примитив есть в процессоре. Тогда можно использовать ряд Тейлора (учитывая, что в окрестности 0 ряд не определен):

<img src=".\sources\LETI5\sqrt1px-exp.png" alt="sqrt1px-exp" style="zoom:25%;" />

Обеспечивается сходимости порядка 1 в $|x|\le 1$

Ограниченность интервала сходимости не является проблемой - в функциях часто аргумент приводится к интервалу, где сходимость быстрее.

Теперь запишем уравнение в виде $x = \sqrt{a}$.  Тогда $x = \varphi(x) = \dfrac{a}{x}$. Но метод неподвижной точки не сходится.

Запишем уравнение в другом виде:

$f(x)=a-x^2=0$

Решим уравнение методом Ньютона:

$\varphi(x)=x-\dfrac{f(x)}{x'(x)} = x + \dfrac{a-x^2}{2x} = 1/2\left(x+\dfrac{a}{x}\right)$

Данный метод называется Вавилонским методом. Сходимость порядка 2, но на каждой итерации происходит деление - операция крайне не желательная в вычислительных алгоритмах по двум причинам:

* Деление занимает намного больше процессорного времени, чем умножение
* Деление может порождать численные артефакты, если знаменатель очень близок к 0

##### Вычисления обратного квадратного корня

Сформулируем задачу иначе:

$x = 1 / \sqrt a$

$f(x)=ax^2-1=0$

Решим уравнение методом Ньютона:

$\varphi(x) = x - \dfrac{ax^2 - 1}{2ax} = x - \dfrac{x}{2}(ax^2 - 1)$

Метод сходится с порядком 2 и не содержит делений. Если начальная точка задана в диапазоне от 0 до 2-х, то метод сходится примерно за 3 итерации, то есть достигает точности, сравнимой с машинной точностью представления числа.

Остается лишь умножить на $a$ полученное значение:

$\sqrt a = a \cdot \dfrac{1}{\sqrt a} = ax$