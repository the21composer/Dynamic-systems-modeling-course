# Методы Адамса

Методы Адамса являются, в отличие от одношаговых методов Рунге-Кутты, многошаговыми.

### Задача (многошаговый метод общего вида)

Пусть в некоторый момент $t$ известно состояние системы $x(t)$. Требуется найти состояние системы на следующем шаге, то есть $x(t+h)$. При этом, в многошаговых методах также известны состояния системы на предыдущих $k - 1$ шагах.

<img src=".\sources\LETI4\xtplush.png" alt="xtplush" style="zoom:25%;" />



Здесь $\dot{x} = \textbf{f}(x)$.

Введем упрощающую нотацию. Пусть $x_n$ – состояние системы на текущем шаге, $\textbf{f}_n$ – вектор правой части для этого состояния. Получаем запись:

<img src=".\sources\LETI4\xnfn.png" alt="xnfn" style="zoom:25%;" />

Так же как и в методах Рунге-Кутты, функция $\textbf{f}$ может вызываться неограниченное число раз.

### Задача (метод Адамса)

Ранее был рассмотрен общий вид многошаговых методов. В методе Адамса же используются предыдущие значения $x_{n-1}, ..., x_{n-k+1}$. Записать это можно так:

<img src=".\sources\LETI4\xnfn2.png" alt="xnfn2" style="zoom:25%;" />

В основе методов Адамса лежит экстраполяция (или интерполяция) функции $\textbf{f}$, то есть правой части.

Допустим, есть истинная функция $\textbf{f}$, проходящая через точки $(t_{n-k+1}, \textbf{f}_{n-k+1}), ..., (t_n, \textbf{f}_{n})$. Функцию можно аппроксимировать полиномом, который также проходит через эти точки, ожидая, что на интервале $t_n, t_{n+1}$ поведение полинома не будет сильно отличаться от поведения функции. 

> Значение следующего $x_{n+1}$ получается путем интегрирования функции $\textbf{f}$ на всем промежутке от $t_n, t_{n+1}$

##### Экстраполяция $\textbf{f}$ на основе $k$ предыдущих значений

Обратные разделенные разности:

<img src=".\sources\LETI4\fn-newtondiff.png" alt="fn" style="zoom:25%;" />

Так, обратная разделенная разность 0-го порядка в точке $\textbf{f}_n$ равна значению $\textbf{f}_n$, а разности высших порядков вычисляются рекурсивно.

С помощью обратных разделенных разностей можно записать интерполяционный многочлен в форме Ньютона:

<img src=".\sources\LETI4\ft-approx.png" alt="ft-approx" style="zoom:25%;" />

Равенство нестрогое, потому что многочлен является аппроксимацией истинной функции $\textbf{f}(t)$.

Введем также нотацию обратных конечных разностей:

<img src=".\sources\LETI4\nabla-f.png" alt="nabla-f" style="zoom:25%;" />

Как видно, нотация похожа на обратные разделенные разности, так же рекурсивно задается. На самом деле, обратную конечную разность можно вывести из обратной разделенной и наоборот:

$t_m - t_{m-i} = ih \Rightarrow [\textbf{f}_n \ ... \ \textbf{f}_{n-i}] = \dfrac{\Delta^i \textbf{f}}{i!h^i}$

Перепишем интерполяционный полином с использованием обратных конечных разностей. Дополнительно введем переменную $s=\dfrac{t-t_n}{h}$

<img src=".\sources\LETI4\ft-approx-finite.png" alt="ft-approx-finite" style="zoom:25%;" />

##### Интеграл правой части

Прежде, чем взять интеграл, перепишем функцию в виде суммы:

<img src=".\sources\LETI4\ft-sum.png" alt="ft-sum" style="zoom:25%;" />

Проинтегрируем функцию от $t_n$ до $t_{n+1}$:

<img src=".\sources\LETI4\xtplus1.png" alt="xtplus1" style="zoom:25%;" />

<img src=".\sources\LETI4\xn-integral-sum.png" alt="xn-integral-sum" style="zoom:25%;" />

Здесь вводятся коэффициенты $\gamma_i$:

<img src=".\sources\LETI4\gamma-i.png" alt="gamma-i" style="zoom:25%;" />

>  Подчеркнем, что при $i=0$ под интегралом только $1$. Соответственно, при $i=1$ под интегралом $s$ и так далее.

Чтобы вывести эти коэффициенты (которые для каждого $i$ являются константой), введем понятие *биномиального коэффициента (обобщенного)*:

<img src=".\sources\LETI4\binomial.png" alt="binomial" style="zoom:25%;" />

Тогда коэффициенты $\gamma_i$ можно записать следующим образом:

<img src=".\sources\LETI4\gamma-i-2.png" alt="gamma-i-2" style="zoom:25%;" />

$(-1)^i$ появилась потому, что каждая скобка была умножена на $-1$ для соответствия биномиальному коэффициенту.

Чтобы посчитать $\gamma_i$, обратимся к методу производящей функции.

Рассмотрим такую функцию $G(u)$:

<img src=".\sources\LETI4\gu.png" alt="gu" style="zoom:25%;" />

Здесь сумма под интегралом – бином Ньютона, обобщенный на случай нецелой степени, поэтому

<img src=".\sources\LETI4\1-u-s.png" alt="1-u-s" style="zoom:25%;" />

Перепишем равенство в другом виде:

<img src=".\sources\LETI4\gu-final.png" alt="gu-final" style="zoom:25%;" />

Все сомножители равенства имеют выражение в рядах, и выражение принимает вид:

<img src=".\sources\LETI4\gu-expanded.png" alt="gu-expanded" style="zoom:25%;" />

Отсюда можно найти $\gamma_0, \gamma_1, ...$

<img src=".\sources\LETI4\gamma-i-recur.png" alt="gamma-i-recur" style="zoom:25%;" />

Решение этой системы уравнений можно представить в виде таблицы, которая как правило заложена в реализации методов:

<img src=".\sources\LETI4\gammas-table.png" alt="gammas-table" style="zoom:25%;" />

##### Метод Адамса-Башфорта: итоговые формулы

Общая формула метода:

<img src=".\sources\LETI4\ab.png" alt="ab" style="zoom:25%;" />

Формулы для $\textbf{x}_{n+1}$ при разном значении $k$ (метод в каждом случае называется $k$-шаговым):

$k=1: \mathbf{x}_{n+1} \approx \mathbf{x} + h \mathbf{f}_n$

$k=2: \mathbf{x}_{n+1} \approx \mathbf{x}_n + h \left( \dfrac{3}{2} \mathbf{f}_n - \dfrac{1}{2} \mathbf{f}_{n-1} \right)$

$k=3: \mathbf{x}_{n+1} \approx \mathbf{x}_n + h \left( \dfrac{23}{12} \mathbf{f}_n - \dfrac{4}{3} \mathbf{f}_{n-1} + \dfrac{5}{12} \mathbf{f}_{n-2} \right)$

$k=4: \mathbf{x}_{n+1} \approx \mathbf{x}_n + h \left( \dfrac{55}{24} \mathbf{f}_n - \dfrac{59}{24} \mathbf{f}_{n-1} + \dfrac{37}{24} \mathbf{f}_{n-2} - \dfrac{3}{8} \mathbf{f}_{n-3} \right)$

Стоит отметить, что методы Адамса не способны работать без использования другого одношагового метода для "разгона". Разгон необходим потому, что метод Адамса подразумевает наличие $k-1$ предыдущих уже сделанных шагов. Соответственно, на первом шаге метода делается $k-1$ шагов одношаговым методом.

Методы Адамса-Башфорта основаны на экстраполяции функции $f$ на один шаг дальше некоторого промежутка, где есть сохраненные ранее значения. Но экстраполяция работает хуже, чем интерполяция, за пределами интервала, в котором имеются известные значения.

 ### Неявный метод Адамса: интерполяция функции правой части

Неявные методы отличаются тем, что используют в определении полинома, который интерполирует функцию $f$, точку $t_{n+1}$.

Интерполяционный полином записывается таким образом и имеет $k+1$ слагаемых (запись с помощью обратных разностей):

<img src=".\sources\LETI4\ft-approx-implicit.png" alt="ft-approx-implicit" style="zoom:25%;" />

#### Метод Адамса-Мултона: вывод коэффициентов

<img src=".\sources\LETI4\xtplus1.png" alt="xtplus1" style="zoom:25%;" />

<img src=".\sources\LETI4\xn-gammastar.png" alt="xn-gammastar" style="zoom:25%;" />

Заметим, что сумма уже от $0$ до $k$ включительно, в отличие от явного метода. Также вводятся коэффициенты $\gamma_i^*$, которые определены следующим образом:

<img src=".\sources\LETI4\gamma-star-int.png" alt="gamma-star-int" style="zoom:25%;" />

Теперь попробуем посчитать разность $\gamma_i - \gamma_{i-1}$ (без звездочек!)

$$
\gamma_i - \gamma_{i-1} = \int_0^1 \left(\begin{array}{c}-s\\ i\end{array}\right) +  \left(\begin{array}{c}-s\\ i - 1\end{array}\right)\mathrm{d}s(-1)^i = \int_0^1 \left(\begin{array}{c}-s+1\\ i\end{array}\right)ds = \gamma_i^*, i>0
$$
Итак, коэффициенты $\gamma_i^*$ можно вывести из $\gamma_i$ по формуле выше, а значит можно аналогично составить таблицу:

<img src=".\sources\LETI4\gammastar-table.png" alt="gammastar-table" style="zoom:25%;" />

##### Предиктор и корректор

Один из методов решения неявных уравнений вида, который задан формулой $\textbf{x}_{n+1}$ - это метод неподвижной точки, которому однако необходимо начальное приближение точки $\textbf{x}_{n+1}$, для чего можно использовать метод Адамса-Башфорта.

Используем нотацию не совсем математическую: далее знак $\leftarrow$ означает присвоение переменной значения.

<img src=".\sources\LETI4\predictor2.png" alt="predictor" style="zoom:25%;" /> - Predictor (P)

<img src=".\sources\LETI4\evalutator2.png" alt="evaluator" style="zoom:25%;" /> - Evaluator (PE)

Итак, теперь можно использовать формулу метода Адамса-Мултона:

<img src=".\sources\LETI4\corrector2.png" alt="corrector" style="zoom:25%;" /> - Corrector (PEC)

Можно предпринимать и дальнейшие шаги:

<img src=".\sources\LETI4\evalutator2.png" alt="evaluator" style="zoom:25%;" />  (PECE)

<img src=".\sources\LETI4\corrector2.png" alt="corrector" style="zoom:25%;" /> (PECEC)

И так далее. Как правило, таких шагов не требуется слишком много, поэтому итерация сходится за 3-4 шага.

Изложенная схема является самодостаточной реализацией неявного метода Адамса, не считая того, что первые $k-1$ значений $f_i$ должны быть, как и в явном методе Адамса, предварительно рассчитаны одношаговым методом.

##### Оптимизация вычислений

Для оптимизации вычислений можно упросить вычисления корректора. Повторим сначала всю схему предиктора и корректора:

<img src=".\sources\LETI4\predictor2.png" alt="predictor" style="zoom:25%;" /> - Predictor (P)

<img src=".\sources\LETI4\evalutator2.png" alt="evaluator" style="zoom:25%;" /> - Evaluator (PE)

<img src=".\sources\LETI4\corrector2.png" alt="corrector" style="zoom:25%;" /> - Corrector (PEC)

На шаге корректора можно ввести поправку, вычислив разность между суммами в корректоре и предикторе:

<img src=".\sources\LETI4\optimiz.png" alt="optimiz" style="zoom:25%;" />

Таким образом, корректор может работать намного проще:

<img src=".\sources\LETI4\optimiz2.png" alt="optimiz2" style="zoom:25%;" />

Используя такой корректор, можно значительно поднять производительность реализации метода.

#### Оценка ошибки методов Адамса

Оценка ошибки выводится из оценки ошибки интерполяции или экстраполяции функции $f$. 

Рассмотрим на примере метода Адамса-Башфорта, то есть на примере экстраполяции (для интерполяции схема такая же).

Посчитаем разность между функцией и ее аппроксимацией:

<img src=".\sources\LETI4\error-estimate-1.png" alt="error-estimate-1" style="zoom:25%;" />

<img src=".\sources\LETI4\xi-span.png" alt="xi-span" style="zoom:25%;" />

Если $\textbf{f}^{(k)}$ ограничена, то ошибка также ограничена:

<img src=".\sources\LETI4\error-estimate-1.png" alt="error-estimate-1" style="zoom:25%;" />

<img src=".\sources\LETI4\error-estimate-2.png" alt="error-estimate-2" style="zoom:25%;" />

##### Локальная оценка ошибки явного метода

<img src=".\sources\LETI4\error-estimate-explicit.png" alt="error-estimate-explicit" style="zoom:25%;" />

##### Локальная оценка ошибки неявного метода

<img src=".\sources\LETI4\error-estimate-implicit.png" alt="error-estimate-implicit" style="zoom:25%;" />

Порядок интерполяции на 1 больше, так как она делается по $k+1$ точке.

Итак,

* $k$-шаговый метод Адамса-Башфорта имеет порядок $k$
* $k$-шаговый метод Адамса-Мултона имеет порядок $k+1$

### Заключение

Стоит сказать, что методы Адамса-Башфорта и Адамса-Мултона придумал именно Адамс, единолично. Они так названы для различимости.

Неявные методы Адамса-Мултона могут накапливать существенно меньшую ошибку, так как используют полиномиальную интерполяцию, а не экстраполяцию.

Общеизвестные источники приводят не самую эффективную реализацию, так как необходимо

* Использовать эффективные методы пересчета конечных разностей
* Эффективные алгоритмы применения корректора

Все методы Адамса не могут работать без одношагового разгона, для него можно использовать методы Рунге-Кутты, например, 4-го порядка или выше.

Выше рассматривалась интерполяция и экстраполяция на равномерной сетке, что может приводить к большим ошибкам. Известна функция Рунге, которая интерполируется тем хуже, чем меньше шаг. Для борьбы с этим можно применять неравномерные сетки, которые затруднительно применять в методах Адамса.

Существуют и другие многошаговые методы, например, метод BDF.

