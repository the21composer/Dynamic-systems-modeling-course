# Дифференцирование

Задача дифференцирования может быть решена разными подходами. Несмотря на то, что задача дифферецирования кажется более простой, чем, например, задача интегрирования, в современных инженерных и научных задачах проблемы, связанные с дифференцированием, актуальны не менее, чем проблемы интегрирования.

<img src=".\sources\LETI8\xkcd.png" alt="xkcd" style="zoom:67%;" />

Рассмотрим несколько задач, где возникает необходимость дифференцирования.

### Применение дифференцирования

В задаче гидродинамики для расчета течения жидкости в некоторой области, дискретизированной сеткой, необходимо решать систему уравнений, в матрице которой находятся производные потока жидкости по числовым характеристикам в узлах сетки (например, по давлению жидкости).

<img src=".\sources\LETI6\cc-grid.png" alt="cc-grid" style="zoom: 33%;" /><img src=".\sources\LETI6\fvm-matrix.png" alt="fvm-matrix" style="zoom:50%;" />

Процесс описывает следующее уравнение:

<img src=".\sources\LETI6\cfd.png" alt="cfd" style="zoom:25%;" />

Так как матрица состоит из производных, то дифференцирование уравнений жидкости, которые даны, это первоочередная задача, возникающая в данной области.

Не менее актуально дифференцирование при нахождении корня системы линейных уравнений, например, при применении метода Ньютона или метода наименьших квадратов.

#### Повторение из лекции про методы решения СЛАУ

$$
\mathbf{f}(\mathbf{x}) = 0,\ \ \mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^n \\
\mathbf{f}(\mathbf{x}) = \mathbf{f}(\mathbf{x_0}) + \nabla \mathbf{f}(\mathbf{x} - \mathbf{x_0}) + O(||\mathbf{x}-\mathbf{x_0}||^2)
$$

Здесь $\nabla \mathbf{f(x)}$ – градиент функции:
$$
\nabla \mathbf{f} = \begin{bmatrix}
    \dfrac{\partial \mathbf{f}}{\partial x_1} & \cdots & \dfrac{\partial \mathbf{f}}{\partial x_n} \end{bmatrix}
= \begin{bmatrix}
    \dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n}\\
    \vdots & \ddots & \vdots\\
    \dfrac{\partial f_n}{\partial x_1} & \cdots & \dfrac{\partial f_n}{\partial x_n} \end{bmatrix}
$$
Поскольку функция $n$-значная и $\mathbf{x}$ – вектор длины $n$, то матрица получается квадратной. Эту матрицу называют *якобиан*.

Невязки – разности значений модели и наблюдений в точках, а минимизируется сумма квадратов этих разностей.
$$
\begin{array}{rcl}
\underset{\mathbf{x}}{\mathrm{arg\,min}}\,S(\mathbf{x}) &=& ? \\
S(\mathbf{x}) &=& \sum_{j=1}^m r_j^2(\mathbf{x})
\end{array}
$$

$$
\nabla S(\mathbf{x}) = \mathbf{0} \Rightarrow f_i(\mathbf{x}) = \sum_{j=1}^m  \frac{\partial r_j(\mathbf{x})}{\partial x_i} r_j(\mathbf{x}) = 0, \quad i = 1..n
\\
\nabla \mathbf{f} \approx A^{\mathrm{T}}A \\
A
= \begin{bmatrix}
    \dfrac{\partial r_1}{\partial x_1} & \cdots & \dfrac{\partial r_1}{\partial x_n}\\
    \vdots & \ddots & \vdots\\
    \dfrac{\partial r_m}{\partial x_1} & \cdots & \dfrac{\partial r_m}{\partial x_n} \end{bmatrix}
$$

>  Более подробно см. пункт «Задачи, в которых появляются СЛАУ» в вышеуказанной лекции

### Применение дифференцирования (продолжение)

Продолжая тему модели и наблюдательных данных, если модель динамическая, то для составления матрицы $A$, чтобы подогнать состояние динамической системы в произвольные моменты времени к наблюдательным данным, необходимо на этапе численного интегрирования уравнений динамической системы знать частные производные от всех ее правых частей по всем переменным ее состояния.

<img src=".\sources\LETI8\dynamic.png" alt="dynamic" style="zoom: 67%;" />

В задачах, связанных с нейронными сетями, в период обучения даются некоторые входные данные и некоторые выходные, и требуется подстроить параметры нейронов (веса), чтобы входные данные соответствовали выходным. Для всего этого требуется подсчитывать большое количество градиентов, которые отражают ход вычислений между нейронами.

<img src=".\sources\LETI8\neural.png" alt="neural" style="zoom: 67%;" />

Итак, можно сказать, что современные компьютеры большую часть времени занимаются решением СЛАУ, либо дифференцированием для их составления, рассмотрим способы выполнения дифференцирования.

### Виды дифференцирования

Рассмотрим функцию:
$$
f(x) = \cos 2x + 2 \sin^2 x
$$

1. Символное дифференцирование дает результат: $f’(x)=0$. Это математический способ, который можно выполнять на бумаге или на компьютере. Несмотря на простой алгоритм, символьным дифференцированием занимаются преимущественно системы компьютерной алгебры, а программы для инженерных и научных расчетов занимаются им в меньшей степени.
2. Численное дифференцирование. Здесь функция $f(x)$ для алгоритмов является черным ящиком.

   $f'(x) = \frac{f(x + h) - f(x)}{h} + O(h)$							(1)

Схему можно немного модернизировать и повысить точность результата:

$f'(x) = \frac{f(x + h) - f(x - h)}{2h} + O(h^2)$						(2)

3. Автоматическое дифференцирование. Данный способ заключается в применении правила взятия полной производной до тех пор, пока его можно применять.

$$
f'(x) = \sin 2x \cdot 2 + 2 (2\sin x \cdot \cos x)
$$

В случае применения для заданной функции видов дифференцирования, указанных в п.2 и п.3, в результате не получится 0.

Поговорим о преимуществах и недостатках каждого из видов дифференцирования.

**Символьное дифференцирование**

Достоинство: наиболее точное (потенциально)

Недостатки: 

* не устраняет дублирование вычислений

Пример:

<img src=".\sources\LETI8\wolfram2.png" alt="wolfram2" style="zoom:50%;" />

Не учитывается, что выражения могут повторяться, что можно устранить дублирование введением временных переменных

* сложно формулируется при наличии ветвлений и циклов

Если в формуле есть суммы, ветвления внутри сумм или просто ветвления, то есть они являются частью вычислительной схемы, то символьное дифференцирование становится непрактичным.

Примеры:

<img src=".\sources\LETI8\gravpot.png" alt="gravpot" style="zoom: 45%;" />

**Численное дифференцирование**

Достоинство: просто в реализации

Недостатки:

* наиболее затратно по времени
* может выходить за область определения $f$

Например, $f(x) = \sqrt{-x}$. При нахождении производной в точке 0 посчитать производную по формуле численного дифференцирования невозможно.

![sqrt](.\sources\LETI8\sqrt.png)

Решением может быть проверка области определения и изменение схемы взятия производной или приравнивание ее к 0.

Особенность:

* выдает ненулевой результат в локальном экстремуме $f$

![x2](.\sources\LETI8\x2.png)

Если для данной параболы искать производную в точке 0 по формуле (1), указанной ниже, то значение будет ненулевым.

Можно использовать измененную схему (2), но в большинстве численных методов используется именно первая схема.

Тем не менее, эта особенность не является проблемой в некоторых ситуациях, а может быть даже и преимуществом, потому что происходит косвенный учет членов второго порядка, что можно назвать компенсацией того, что численные методы отбрасывают члены второго порядка, и, оказавшись в точке локального экстремума, не будут «считать», что выход из точки не приведет к изменению функции.

**Автоматическое дифференцирование**

Недостаток: сложно в реализации

Достоинства:

* наиболее быстрое за счет исключения дублирующих вычислений
* легко формируется при наличии циклов, ветвлений, подпроцедур, временных переменных и пр.

#### Пример

Уравнение Кеплера:
$$
M=E-e\sin E
$$
Программный код, соответствующий этой формуле:

<img src=".\sources\LETI8\kepler3.png" alt="kepler3" style="zoom:50%;" />

Здесь $w_i$ – временные переменные.

Схематично процедуру можно изобразить так:

<img src=".\sources\LETI8\kepler1.png" alt="kepler1" style="zoom:50%;" />

Дифференцирование будем производить по обоим входным параметрам.

<img src=".\sources\LETI8\dm1.png" alt="dm1" style="zoom:50%;" />                    <img src=".\sources\LETI8\dm2.png" alt="dm2" style="zoom:50%;" />

Функция $\mathrm{d}M$ перед присваиванием в каждую из временных переменных делается присваивание двух (их может быть и больше) частных производных. Также видно, что для вычисления новых частных производных можно использовать уже посчитанные ранее, а также сами значения временных переменных.

Данная схема называется прямым автоматическим дифференцированием, оно уместно в ситуациях, когда присутствует достаточно большое количество входных данных и умеренное количество параметров. 

Есть и схема обратного автоматического дифференцирования, которая актуальна для задач, связанных с нейронными сетями, где количество параметров достаточно большое.

Существуют способы комбинировать прямое и обратное автоматическое дифференцирования, так как схемы не гарантируют оптимальной работы алгоритма.

Есть и более глубокие аспекты, связанные с дифференцированием программного кода, например, дифференцирование с рекурсией. Для практических нужд существует множество программных пакетов, реализующих автоматическое дифференцирование.

